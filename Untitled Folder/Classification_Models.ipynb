{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacfe47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, confusion_matrix, jaccard_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851e2b9",
   "metadata": {},
   "source": [
    "### Reading in simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed094282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('sim_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb34bbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prior_exp</th>\n",
       "      <th>usage_hours</th>\n",
       "      <th>publishes</th>\n",
       "      <th>ai</th>\n",
       "      <th>at3</th>\n",
       "      <th>at4</th>\n",
       "      <th>at5</th>\n",
       "      <th>utility</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.890440</td>\n",
       "      <td>250.175218</td>\n",
       "      <td>9.599912</td>\n",
       "      <td>14.051599</td>\n",
       "      <td>1.197865</td>\n",
       "      <td>3.998860</td>\n",
       "      <td>5.004053</td>\n",
       "      <td>27.500887</td>\n",
       "      <td>0.412330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.396343</td>\n",
       "      <td>144.626707</td>\n",
       "      <td>2.198206</td>\n",
       "      <td>3.433038</td>\n",
       "      <td>1.076411</td>\n",
       "      <td>1.000608</td>\n",
       "      <td>2.887509</td>\n",
       "      <td>4.202783</td>\n",
       "      <td>0.492255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446405</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>1.257861</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>124.657227</td>\n",
       "      <td>9.229995</td>\n",
       "      <td>14.102369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.325622</td>\n",
       "      <td>2.498452</td>\n",
       "      <td>27.036460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>250.366247</td>\n",
       "      <td>9.949248</td>\n",
       "      <td>14.889738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.996904</td>\n",
       "      <td>5.004031</td>\n",
       "      <td>28.553327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>375.300415</td>\n",
       "      <td>10.643674</td>\n",
       "      <td>15.606132</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.673544</td>\n",
       "      <td>7.505106</td>\n",
       "      <td>29.861572</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>499.998308</td>\n",
       "      <td>14.683496</td>\n",
       "      <td>19.357789</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.738615</td>\n",
       "      <td>9.999990</td>\n",
       "      <td>37.191022</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prior_exp    usage_hours      publishes             ai  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.890440     250.175218       9.599912      14.051599   \n",
       "std         0.396343     144.626707       2.198206       3.433038   \n",
       "min         0.000000       0.001127       0.000000       0.000000   \n",
       "25%         2.000000     124.657227       9.229995      14.102369   \n",
       "50%         2.000000     250.366247       9.949248      14.889738   \n",
       "75%         2.000000     375.300415      10.643674      15.606132   \n",
       "max         2.000000     499.998308      14.683496      19.357789   \n",
       "\n",
       "                 at3            at4            at5        utility  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.197865       3.998860       5.004053      27.500887   \n",
       "std         1.076411       1.000608       2.887509       4.202783   \n",
       "min         0.000000      -0.446405       0.000140       1.257861   \n",
       "25%         0.000000       3.325622       2.498452      27.036460   \n",
       "50%         1.000000       3.996904       5.004031      28.553327   \n",
       "75%         2.000000       4.673544       7.505106      29.861572   \n",
       "max         3.000000       8.738615       9.999990      37.191022   \n",
       "\n",
       "          conversion  \n",
       "count  200000.000000  \n",
       "mean        0.412330  \n",
       "std         0.492255  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffcd705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 0 to 199999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   publishes   200000 non-null  float64\n",
      " 1   ai          200000 non-null  float64\n",
      " 2   at3         200000 non-null  int64  \n",
      " 3   at4         200000 non-null  float64\n",
      " 4   at5         200000 non-null  float64\n",
      " 5   conversion  200000 non-null  int64  \n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 10.7 MB\n"
     ]
    }
   ],
   "source": [
    "drop_cols = ['prior_exp','usage_hours','utility']\n",
    "df = df.drop(columns=drop_cols)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff9de2",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6e13c",
   "metadata": {},
   "source": [
    "Splitting the data into a training dataset and test dataset.  We will train on the training set and evalute predictive power on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b563ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['conversion']\n",
    "x = df.drop(columns='conversion')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a6720",
   "metadata": {},
   "source": [
    "### Splitting the training data into 3 seperate data frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a8321",
   "metadata": {},
   "source": [
    "In addition to just overal predictive power as a benchmark, I am also thinking we can evalute the different methods on how accruate they are with more limited data sizes.  I create 3 databases, one with 10k records, another with 25k records, and a final with 50k records.  I will then train the model on all.  This will allow us to evalute how much better different models get with larger training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe120317",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_10k = x_train[0:10000]\n",
    "x_train_25k = x_train[0:25000]\n",
    "x_train_50k = x_train[0:50000]\n",
    "\n",
    "y_train_10k = y_train[0:10000]\n",
    "y_train_25k = y_train[0:25000]\n",
    "y_train_50k = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713502f8",
   "metadata": {},
   "source": [
    "### Preditive Power Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586149c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metrics(prediction):\n",
    "    a = print(\"Accuracy:\",metrics.accuracy_score(y_test, prediction))\n",
    "    p = print(\"Precision:\",metrics.precision_score(y_test, prediction))\n",
    "    r = print(\"Recall:\",metrics.recall_score(y_test, prediction))\n",
    "    f = print(\"F-1 Score:\",metrics.f1_score(y_test, prediction))\n",
    "    return a, p, r, f;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e02ba",
   "metadata": {},
   "source": [
    "### Method 1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f91f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70561\n",
      "Precision: 0.5895155698911766\n",
      "Recall: 0.9466321368928045\n",
      "F-1 Score: 0.7265634433370797\n",
      "(None, None, None, None)\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "#running first on x_train_10k\n",
    "gn1 = GaussianNB()\n",
    "gnm1 = gn1.fit(x_train_10k, y_train_10k)\n",
    "gnr1 = gnm1.predict(x_test)\n",
    "print(class_metrics(gnr1))\n",
    "print(gnm1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31565bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70924\n",
      "Precision: 0.5930577306107555\n",
      "Recall: 0.9440666069656558\n",
      "F-1 Score: 0.7284849843119677\n",
      "(None, None, None, None)\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "#running second time on x_train_20k\n",
    "gn2 = GaussianNB()\n",
    "gnm2 = gn2.fit(x_train_25k, y_train_25k)\n",
    "gnr2 = gnm2.predict(x_test)\n",
    "print(class_metrics(gnr2))\n",
    "print(gnm2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda52727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70537\n",
      "Precision: 0.5893050867888139\n",
      "Recall: 0.9466079337802842\n",
      "F-1 Score: 0.7263964340437387\n",
      "(None, None, None, None)\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "#running second time on x_train_20k\n",
    "gn3 = GaussianNB()\n",
    "gnm3 = gn3.fit(x_train_50k, y_train_50k)\n",
    "gnr3 = gnm3.predict(x_test)\n",
    "print(class_metrics(gnr3))\n",
    "print(gnm3.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
